{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Use this Jupyter Notebook as a guide to run your trained model in inference mode\n","\n","created by Anton Morgunov\n","\n","inspired by [notebooks object detection API tutorial](https://notebooks-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model)"]},{"cell_type":"markdown","metadata":{},"source":["Your first step is going to specify which unit you are going to work with for inference. Select between GPU or CPU and follow the below instructions for implementation."]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["import os # importing OS in order to make GPU visible\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # do not change anything in here\n","\n","# specify which device you want to work on.\n","# Use \"-1\" to work on a CPU. Default value \"0\" stands for the 1st GPU that will be used\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # TODO: specify your computational device"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No GPU found\n"]},{"name":"stderr","output_type":"stream","text":["2021-10-10 19:28:39.270406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import notebooks as tf # import notebooks\n","\n","# checking that GPU is found\n","if tf.test.gpu_device_name():\n","    print('GPU found')\n","else:\n","    print(\"No GPU found\")"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# other import\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["Next you will import import scripts that were already provided by notebooks API. **Make sure that notebooks is your current working directory.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sys # importyng sys in order to access scripts located in a different folder\n","\n","path2scripts = 'models/research/' # TODO: provide pass to the research folder\n","sys.path.insert(0, path2scripts) # making scripts in models/research available for import"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["# importing all scripts that will be needed to export your model and use it for inference\n","from object_detection.utils import label_map_util\n","from object_detection.utils import config_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.builders import model_builder"]},{"cell_type":"markdown","metadata":{},"source":["Now you can import and build your trained model:"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# NOTE: your current working directory should be notebooks.\n","\n","# specify two pathes: to the pipeline.config file and to the folder with trained model.\n","path2config ='../workspace/exported_models/efficientdet_d0/pipeline.config'\n","path2model = '../workspace/exported_models/efficientdet_d0/'"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["# do not change anything in this cell\n","configs = config_util.get_configs_from_pipeline_file(path2config) # importing config\n","model_config = configs['model'] # recreating model config\n","detection_model = model_builder.build(model_config=model_config, is_training=False) # importing model"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fae217847d0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n","ckpt.restore(os.path.join(path2model, 'checkpoint', 'ckpt-0')).expect_partial()"]},{"cell_type":"markdown","metadata":{},"source":["Next, path to label map should be provided. Category index will be created based on labal map file"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["path2label_map = '../workspace/data/label_map.pbtxt'\n","category_index = label_map_util.create_category_index_from_labelmap(path2label_map,use_display_name=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now, a few supporting functions will be defined"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["def detect_fn(image):\n","    \"\"\"\n","    Detect objects in image.\n","    \n","    Args:\n","      image: (tf.tensor): 4D input image\n","      \n","    Returs:\n","      detections (dict): predictions that model made\n","    \"\"\"\n","\n","    image, shapes = detection_model.preprocess(image)\n","    prediction_dict = detection_model.predict(image, shapes)\n","    detections = detection_model.postprocess(prediction_dict, shapes)\n","\n","    return detections"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["def load_image_into_numpy_array(path):\n","    \"\"\"Load an image from file into a numpy array.\n","\n","    Puts image into numpy array to feed into notebooks graph.\n","    Note that by convention we put it into a numpy array with shape\n","    (height, width, channels), where channels=3 for RGB.\n","\n","    Args:\n","      path: the file path to the image\n","\n","    Returns:\n","      numpy array with shape (img_height, img_width, 3)\n","    \"\"\"\n","    \n","    return np.array(Image.open(path))"]},{"cell_type":"markdown","metadata":{},"source":["**Next function is the one that you can use to run inference and plot results an an input image:**"]},{"cell_type":"code","execution_count":18,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["def inference_with_plot(path2images, box_th=0.25):\n","    \"\"\"\n","    Function that performs inference and plots resulting b-boxes\n","    \n","    Args:\n","      path2images: an array with pathes to images\n","      box_th: (float) value that defines threshold for model prediction.\n","      \n","    Returns:\n","      None\n","    \"\"\"\n","    for image_path in path2images:\n","\n","        print('Running inference for {}... '.format(image_path), end='')\n","\n","        image_np = load_image_into_numpy_array(image_path)\n","        \n","        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","        detections = detect_fn(input_tensor)\n","\n","        # All outputs are batches tensors.\n","        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n","        # We're only interested in the first num_detections.\n","        num_detections = int(detections.pop('num_detections'))\n","        detections = {key: value[0, :num_detections].numpy()\n","                      for key, value in detections.items()}\n","        \n","        detections['num_detections'] = num_detections\n","\n","        # detection_classes should be ints.\n","        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","        label_id_offset = 1\n","        image_np_with_detections = image_np.copy()\n","\n","        viz_utils.visualize_boxes_and_labels_on_image_array(\n","                image_np_with_detections,\n","                detections['detection_boxes'],\n","                detections['detection_classes']+label_id_offset,\n","                detections['detection_scores'],\n","                category_index,\n","                use_normalized_coordinates=True,\n","                max_boxes_to_draw=200,\n","                min_score_thresh=box_th,\n","                agnostic_mode=False,\n","                line_thickness=5)\n","\n","        plt.figure(figsize=(15,10))\n","        plt.imshow(image_np_with_detections)\n","        print('Done')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Next, we will define a few other supporting functions:"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["def nms(rects, thd=0.5):\n","    \"\"\"\n","    Filter rectangles\n","    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n","    thd - intersection threshold (intersection divides min square of rectange)\n","    \"\"\"\n","    out = []\n","\n","    remove = [False] * len(rects)\n","\n","    for i in range(0, len(rects) - 1):\n","        if remove[i]:\n","            continue\n","        inter = [0.0] * len(rects)\n","        for j in range(i, len(rects)):\n","            if remove[j]:\n","                continue\n","            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n","\n","        max_prob = 0.0\n","        max_idx = 0\n","        for k in range(i, len(rects)):\n","            if inter[k] >= thd:\n","                if rects[k][1] > max_prob:\n","                    max_prob = rects[k][1]\n","                    max_idx = k\n","\n","        for k in range(i, len(rects)):\n","            if (inter[k] >= thd) & (k != max_idx):\n","                remove[k] = True\n","\n","    for k in range(0, len(rects)):\n","        if not remove[k]:\n","            out.append(rects[k])\n","\n","    boxes = [box[0] for box in out]\n","    scores = [score[1] for score in out]\n","    classes = [cls[2] for cls in out]\n","    return boxes, scores, classes\n","\n","\n","def intersection(rect1, rect2):\n","    \"\"\"\n","    Calculates square of intersection of two rectangles\n","    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n","    return: square of intersection\n","    \"\"\"\n","    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n","    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n","    overlapArea = x_overlap * y_overlap;\n","    return overlapArea\n","\n","\n","def square(rect):\n","    \"\"\"\n","    Calculates square of rectangle\n","    \"\"\"\n","    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"]},{"cell_type":"markdown","metadata":{},"source":["**Next function is the one that you can use to run inference and save results into a file:**"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["def inference_as_raw_output(path2images,\n","                            box_th = 0.25,\n","                            nms_th = 0.5,\n","                            to_file = False,\n","                            data = None,\n","                            path2dir = False):\n","    \"\"\"\n","    Function that performs inference and return filtered predictions\n","    \n","    Args:\n","      path2images: an array with pathes to images\n","      box_th: (float) value that defines threshold for model prediction. Consider 0.25 as a value.\n","      nms_th: (float) value that defines threshold for non-maximum suppression. Consider 0.5 as a value.\n","      to_file: (boolean). When passed as True => results are saved into a file. Writing format is\n","      path2image + (x1abs, y1abs, x2abs, y2abs, score, conf) for box in boxes\n","      data: (str) name of the dataset you passed in (e.g. test/validation)\n","      path2dir: (str). Should be passed if path2images has only basenames. If full pathes provided => set False.\n","      \n","    Returs:\n","      detections (dict): filtered predictions that model made\n","    \"\"\"\n","\n","    print(f'Current data set is {data}')\n","    print(f'Ready to start inference on {len(path2images)} images!')\n","    \n","    for image_path in tqdm(path2images):\n","        \n","        if path2dir: # if a path to a directory where images are stored was passed in\n","            image_path = os.path.join(path2dir, image_path.strip())\n","            \n","        image_np = load_image_into_numpy_array(image_path)\n","\n","        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","        detections = detect_fn(input_tensor)\n","        \n","        # checking how many detections we got\n","        num_detections = int(detections.pop('num_detections'))\n","        \n","        # filtering out detection in order to get only the one that are indeed detections\n","        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n","        \n","        # detection_classes should be ints.\n","        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","        \n","        # defining what we need from the resulting detection dict that we got from model output\n","        key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n","        \n","        # filtering out detection dict in order to get only boxes, classes and scores\n","        detections = {key: value for key, value in detections.items() if key in key_of_interest}\n","        \n","        if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n","            for key in key_of_interest:\n","                scores = detections['detection_scores']\n","                current_array = detections[key]\n","                filtered_current_array = current_array[scores > box_th]\n","                detections[key] = filtered_current_array\n","        \n","        if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n","            # creating a zip object that will contain model output info as\n","            output_info = list(zip(detections['detection_boxes'],\n","                                   detections['detection_scores'],\n","                                   detections['detection_classes']\n","                                  )\n","                              )\n","            boxes, scores, classes = nms(output_info)\n","            \n","            detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n","            detections['detection_scores'] = scores\n","            detections['detection_classes'] = classes\n","            \n","        if to_file and data: # if saving to txt file was requested\n","\n","            image_h, image_w, _ = image_np.shape\n","            file_name = f'pred_result_{data}.txt'\n","            \n","            line2write = list()\n","            line2write.append(os.path.basename(image_path))\n","            \n","            with open(file_name, 'a+') as text_file:\n","                # iterating over boxes\n","                for b, s, c in zip(boxes, scores, classes):\n","                    \n","                    y1abs, x1abs = b[0] * image_h, b[1] * image_w\n","                    y2abs, x2abs = b[2] * image_h, b[3] * image_w\n","                    \n","                    list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n","                    line2append = ','.join([str(item) for item in list2append])\n","                    \n","                    line2write.append(line2append)\n","                \n","                line2write = ' '.join(line2write)\n","                text_file.write(line2write + os.linesep)\n","        \n","        return detections"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"179d3b3ef181d5f6145a5ad2bfea98fa737e797b1821c794b78497d3c42eb6b7"},"kernelspec":{"display_name":"Python 3.7.10 64-bit ('.venv': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"neptune":{"notebookId":"7c618cd5-39ec-46c6-bee7-0cfe5297f22a"}},"nbformat":4,"nbformat_minor":4}
